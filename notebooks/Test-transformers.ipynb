{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from transformers import  TFDebertaV2Model, DebertaV2TokenizerFast\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers, Input, Sequential, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    base_dir = \"/kaggle/working/\"\n",
    "  \n",
    "    # dataset path   \n",
    "    parent_path=os.path.join(os.getcwd(), os.pardir)\n",
    "    train_dataset_path=os.path.join(parent_path, \"raw_data/train.csv\")\n",
    "    \n",
    "    #tokenizer params\n",
    "    truncation = True\n",
    "    padding = True #'max_length'\n",
    "    max_length = 512\n",
    "    \n",
    "    # model params\n",
    "    train_col='full_text'\n",
    "    model_name = \"microsoft/deberta-v2-xlarge\"\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary',\n",
    "       'phraseology', 'grammar', 'conventions']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tokenizer\n",
    "first, we define a function to tokenize the text from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(\"microsoft/deberta-v2-xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (df):\n",
    "    texts=list(df[config.train_col])\n",
    "    tokenized=tokenizer(texts,\n",
    "                       padding=config.padding,\n",
    "                       truncating=True,\n",
    "                       max_length=config.max_length)\n",
    "    tokenized[\"labels\"]= [df[column] for column in config.target_cols]\n",
    "    tokenized['length'] = len(tokenized['input_ids'])\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(config.train_dataset_path).copy()\n",
    "texts=data[config.train_col]\n",
    "targets=data[config.target_cols]\n",
    "train_texts, val_texts, train_targets, val_targets=train_test_split(texts, targets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3128, 6)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the data into a tokenized form\n",
    "Here we want the tokens to be read by a tf.keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_texts = tokenizer(list(train_texts), return_tensors='tf',truncation=config.truncation, padding=config.padding)\n",
    "tokenized_val_texts = tokenizer(list(val_texts), return_tensors='tf', truncation=config.truncation, padding=config.padding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "Here we use the output of the pretrained DeBerta model as an input of a dense intermediate layer, then we input the result in the linear regression parallele output layers, for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=((512)),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDebertaV2Model.\n",
      "\n",
      "All the layers of TFDebertaV2Model were initialized from the model checkpoint at microsoft/deberta-v2-xlarge.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_deberta_v2_model_15 (TFDebe  TFBaseModelOutput(l  884593152  ['attention_mask[0][0]',         \n",
      " rtaV2Model)                    ast_hidden_state=(N               'input_ids[0][0]']              \n",
      "                                one, 512, 1536),                                                  \n",
      "                                 hidden_states=((No                                               \n",
      "                                ne, 512, 1536),                                                   \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536),                                               \n",
      "                                 (None, 512, 1536))                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 1536)        0           ['tf_deberta_v2_model_15[0][25]']\n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64)           98368       ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " cohesion (Dense)               (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " syntax (Dense)                 (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " vocabulary (Dense)             (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " phraseology (Dense)            (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " grammar (Dense)                (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " conventions (Dense)            (None, 1)            65          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 884,691,910\n",
      "Trainable params: 98,758\n",
      "Non-trainable params: 884,593,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import the needed model(Bert, Roberta or DistilBert) with output_hidden_states=True\n",
    "transformer_model = TFDebertaV2Model.from_pretrained('microsoft/deberta-v2-xlarge', output_hidden_states=True, return_dict=True)\n",
    "transformer_model.trainable = False\n",
    "\n",
    "input_ids = Input(shape=((512)),dtype='int32', name='input_ids')\n",
    "attention_mask = Input(shape=((512)), dtype='int32', name='attention_mask')\n",
    "\n",
    "transformer = transformer_model(dict(input_ids=input_ids,attention_mask=attention_mask))    \n",
    "hidden_states = transformer[0] # get output_hidden_states\n",
    "\n",
    "\n",
    "# Add a layer maxpool 1D\n",
    "pooling_layer = layers.GlobalMaxPooling1D()(hidden_states)\n",
    "\n",
    "# Now we can use selected_hiddes_states as we want\n",
    "last_hidden_layer = layers.Dense(64, activation='relu')(pooling_layer)\n",
    "\n",
    "# Defining the regression layer\n",
    "cohesion_output=layers.Dense(1, activation=\"linear\", name=\"cohesion\")(last_hidden_layer)\n",
    "syntax_output=layers.Dense(1, activation=\"linear\", name=\"syntax\")(last_hidden_layer)\n",
    "vocabulary_output=layers.Dense(1, activation=\"linear\", name=\"vocabulary\")(last_hidden_layer)\n",
    "phraseology_output=layers.Dense(1, activation=\"linear\", name=\"phraseology\")(last_hidden_layer)\n",
    "grammar_output=layers.Dense(1, activation=\"linear\", name=\"grammar\")(last_hidden_layer)\n",
    "conventions_output=layers.Dense(1, activation=\"linear\", name=\"conventions\")(last_hidden_layer)\n",
    "\n",
    "# output in a list\n",
    "output= [cohesion_output, syntax_output, vocabulary_output, phraseology_output, grammar_output, conventions_output]\n",
    "\n",
    "#Assembling the model\n",
    "model = Model(inputs = [input_ids, attention_mask], outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(loss='mse', optimizer='adam',loss_weights=[1/6 for i in range(6)], metrics= root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 8/40 [=====>........................] - ETA: 3:29:09 - loss: 7.9611 - cohesion_loss: 6.4064 - syntax_loss: 14.2089 - vocabulary_loss: 3.5786 - phraseology_loss: 3.6799 - grammar_loss: 4.9086 - conventions_loss: 14.9840 - cohesion_mse: 6.4064 - syntax_mse: 14.2089 - vocabulary_mse: 3.5786 - phraseology_mse: 3.6799 - grammar_mse: 4.9086 - conventions_mse: 14.9840"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "history = model.fit(x={'input_ids':tokenized_train_texts['input_ids'],\n",
    "                        'attention_mask':tokenized_train_texts['attention_mask']},\n",
    "                    y=train_targets,epochs=100,batch_size=64,validation_split=0.2, callbacks=[es],\n",
    "          verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

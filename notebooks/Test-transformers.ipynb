{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from transformers import  TFDebertaV2Model, DebertaV2TokenizerFast\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import layers, Input, Sequential, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class config:\n",
    "    base_dir = \"/kaggle/working/\"\n",
    "    seed = 69\n",
    "    # dataset path \n",
    "    \n",
    "    parent_path=os.path.join(os.getcwd(), os.pardir)\n",
    "    train_dataset_path=os.path.join(parent_path, \"raw_data/train.csv\")\n",
    "       \n",
    "    #save_dir=\"../input/colab-models-download-v2-0/\"\n",
    "    \n",
    "    #tokenizer params\n",
    "    truncation = True\n",
    "    padding = True #'max_length'\n",
    "    max_length = 512\n",
    "    \n",
    "    # model params\n",
    "    train_col='full_text'\n",
    "    model_name = \"microsoft/deberta-v3-large\"\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary',\n",
    "       'phraseology', 'grammar', 'conventions']\n",
    "    load_from_disk = None\n",
    "    \n",
    "    #training params\n",
    "    learning_rate = 9e-6\n",
    "    batch_size = 2\n",
    "    epochs = 3\n",
    "    NFOLDS = 5\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tokenizer\n",
    "first, we define a function to tokenize the text from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (df):\n",
    "    texts=list(df[config.train_col])\n",
    "    tokenized=tokenizer(texts,\n",
    "                       padding=config.padding,\n",
    "                       truncating=True,\n",
    "                       max_length=config.max_length)\n",
    "    tokenized[\"labels\"]= [df[column] for column in config.target_cols]\n",
    "    tokenized['length'] = len(tokenized['input_ids'])\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(\"microsoft/deberta-v2-xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(config.train_dataset_path).copy()\n",
    "texts=data[config.train_col]\n",
    "targets=data[config.target_cols]\n",
    "train_texts, val_texts, train_targets, val_targets=train_test_split(texts, targets, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the data into a tokenized form\n",
    "Here we want the tokens to be read by a tf.keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_texts = tokenizer(list(train_texts), return_tensors='tf',truncation=config.truncation, padding=config.padding)\n",
    "tokenized_val_texts = tokenizer(list(val_texts), return_tensors='tf', truncation=config.truncation, padding=config.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_texts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3128, 512), dtype=int32, numpy=\n",
       "array([[   1, 3031,  334, ...,    0,    0,    0],\n",
       "       [   1, 2074,  189, ...,    0,    0,    0],\n",
       "       [   1,   16, 8455, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   16,  214, ...,    0,    0,    0],\n",
       "       [   1,   23,  569, ...,    0,    0,    0],\n",
       "       [   1,   23,  307, ...,    0,    0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3128, 512), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3128, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3128, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_texts['input_ids'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "Here we use the output of the pretrained DeBerta model as an input of a dense intermediate layer, then we input the result in the linear regression parallele output layers, for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=((512)),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"tf_model.h5\";: 100%|██████████| 3.54G/3.54G [05:07<00:00, 11.5MB/s]\n",
      "2023-03-08 15:28:31.208272: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 787046400 exceeds 10% of free system memory.\n",
      "2023-03-08 15:28:33.736233: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 787046400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import the needed model(Bert, Roberta or DistilBert) with output_hidden_states=True\n",
    "transformer_model = TFDebertaV2Model.from_pretrained('microsoft/deberta-v2-xlarge', output_hidden_states=True)\n",
    "\n",
    "input_ids = Input(shape=((512)),dtype='int32')\n",
    "attention_mask = Input(shape=((512)), dtype='int32')\n",
    "\n",
    "transformer = transformer_model(dict(input_ids=input_ids,attention_mask=attention_mask))    \n",
    "hidden_states = transformer[1] # get output_hidden_states\n",
    "\n",
    "hidden_states_size = 4 # count of the last states \n",
    "hiddes_states_ind = list(range(-hidden_states_size, 0, 1))\n",
    "\n",
    "selected_hiddes_states = layers.concatenate(tuple([hidden_states[i] for i in hiddes_states_ind]))\n",
    "\n",
    "# Now we can use selected_hiddes_states as we want\n",
    "last_hidden_layer = layers.Dense(8, activation='relu')(selected_hiddes_states)\n",
    "\n",
    "# Defining the regression layer\n",
    "cohesion_output=layers.Dense(1, activation=\"linear\", name=\"cohesion\")(last_hidden_layer)\n",
    "syntax_output=layers.Dense(1, activation=\"linear\", name=\"syntax\")(last_hidden_layer)\n",
    "vocabulary_output=layers.Dense(1, activation=\"linear\", name=\"vocabulary\")(last_hidden_layer)\n",
    "phraseology_output=layers.Dense(1, activation=\"linear\", name=\"phraseology\")(last_hidden_layer)\n",
    "grammar_output=layers.Dense(1, activation=\"linear\", name=\"grammar\")(last_hidden_layer)\n",
    "conventions_output=layers.Dense(1, activation=\"linear\", name=\"conventions\")(last_hidden_layer)\n",
    "\n",
    "# output in a list\n",
    "output= [cohesion_output, syntax_output, vocabulary_output, phraseology_output, grammar_output, conventions_output]\n",
    "\n",
    "#Assembling the model\n",
    "model = Model(inputs = [input_ids, attention_mask], outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
